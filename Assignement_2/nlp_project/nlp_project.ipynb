{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project - Paul Asquin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        \n",
    "        # Patching id2word\n",
    "        for i, key in enumerate(self.word2vec.keys()):\n",
    "            self.word2id[key]=i    \n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        \n",
    "        # Computing scores\n",
    "        scores = []\n",
    "        for dic_word in self.word2vec:\n",
    "            scores.append(self.score(w, dic_word))\n",
    "            \n",
    "        # Extracting the K best ids (K last with sorted list)\n",
    "        id_K_most_similar = np.argsort(scores)[::-1][0:K]\n",
    "    \n",
    "        # Getting their names\n",
    "        word_K_most_similar = []\n",
    "        for id_word in id_K_most_similar:\n",
    "            word_K_most_similar.append(self.id2word[id_word])\n",
    "\n",
    "        return word_K_most_similar\n",
    "        \n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if w1 in self.word2vec.keys() and w2 in self.word2vec.keys():\n",
    "            w1_emb = self.word2vec[w1]\n",
    "            w2_emb = self.word2vec[w2]\n",
    "            return np.dot(w1_emb, w2_emb) / (np.linalg.norm(w1_emb) * np.linalg.norm(w2_emb))\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'canine']\n",
      "['dogs', 'dog', 'Dogs', 'puppies', 'cats']\n",
      "['paris', 'france', 'Paris', 'london', 'europe']\n",
      "['germany', 'europe', 'german', 'france', 'Germany']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=30000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))\n",
    "    \n",
    "# NOTE : On teacher's adive, used nmax = 30000 to obtain coherent results for paris, france, germany and berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors \n",
    "                embeddings = []\n",
    "                for word in sent:\n",
    "                    if word in self.w2v.word2vec:\n",
    "                        embeddings.append(self.w2v.word2vec[word])\n",
    "                if len(embeddings) == 0:\n",
    "                    sentemb.append(np.zeros(list(self.w2v.word2vec.values())[0].shape))\n",
    "                else:\n",
    "                    sentemb.append(np.mean(embeddings, axis=0))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                embeddings = []\n",
    "                for word in sent:\n",
    "                    if word in self.w2v.word2vec:\n",
    "                        embeddings.append(idf[word] * self.w2v.word2vec[word])\n",
    "                if len(embeddings) == 0:\n",
    "                    sentemb.append(np.zeros(list(self.w2v.word2vec.values())[0].shape))\n",
    "                else:\n",
    "                    sentemb.append(np.mean(embeddings, axis=0))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        \n",
    "        scores = []\n",
    "        similar_sentences = []\n",
    "        # Computing scores\n",
    "        for id_sentence in sentences:\n",
    "            scores.append(self.score(s, id_sentence, idf))\n",
    "        \n",
    "        # Extracting the K best ids (K last with sorted list)\n",
    "        id_K_most_similar = (np.argsort(scores)[::-1][0:K])\n",
    "    \n",
    "        # Getting the sentences\n",
    "        for id_similar_sentence in id_K_most_similar:\n",
    "            similar_sentences.append(sentences[id_similar_sentence])\n",
    "        print(\"\\nSimilar sentences to \", \" \".join(s), \" :\") \n",
    "        print(\"\\n\".join([\" \".join(sentence) for sentence in similar_sentences]))\n",
    "        return similar_sentences\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        s1_emb = self.encode([s1], idf)\n",
    "        s2_emb = self.encode([s2], idf)\n",
    "        if np.linalg.norm(s1_emb) != 0 and np.linalg.norm(s2_emb)!=0:\n",
    "            return np.dot(s1_emb, s2_emb.T) / (np.linalg.norm(s1_emb) * np.linalg.norm(s2_emb))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        for word in idf:\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "\n",
      "Similar sentences to  1 smiling african american boy .  :\n",
      "1 smiling african american boy .\n",
      "blond boy waterskiing .\n",
      "a boy jumps .\n",
      "a boy jumps .\n",
      "a boy smiles underwater .\n",
      "\n",
      "Similar sentences to  1 smiling african american boy .  :\n",
      "1 smiling african american boy .\n",
      "5 women and 1 man are smiling for the camera .\n",
      "a man rides a 4 wheeler in the desert .\n",
      "3 males and 1 woman enjoying a sporting event\n",
      "a man in black is juggling 3 flamed bottles .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.59633357]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with open(os.path.join(PATH_TO_DATA,'sentences.txt')) as file_sentences:\n",
    "    for line in file_sentences:\n",
    "        sentences.append(line.replace(\" \\n\", \"\").replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "# Build idf scores for each word\n",
    "#idf = {} if True else s2v.build_idf(sentences)\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "# idf = {}  \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English\n",
      "Loaded 50000 pretrained word vectors\n",
      "Loading French\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "print(\"Loading English\")\n",
    "word2vec_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "print(\"Loading French\")\n",
    "word2vec_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n",
    "\n",
    "# Patching id2word\n",
    "for i, key in enumerate(word2vec_fr.word2vec.keys()):\n",
    "    word2vec_fr.word2id[key]=i    \n",
    "word2vec_fr.id2word = {v: k for k, v in word2vec_fr.word2id.items()}\n",
    "\n",
    "for i, key in enumerate(word2vec_en.word2vec.keys()):\n",
    "    word2vec_en.word2id[key]=i    \n",
    "word2vec_en.id2word = {v: k for k, v in word2vec_en.word2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# shared words 18970\n",
      "X shape :  (300, 18970)\n",
      "Y shape :  (300, 18970)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "shared_words = []\n",
    "for word in word2vec_en.word2vec:\n",
    "    if word in word2vec_fr.word2vec:\n",
    "        shared_words.append(word)\n",
    "\n",
    "print(\"# shared words\", len(shared_words))\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for word in shared_words:\n",
    "    X.append(word2vec_en.word2vec[word])\n",
    "    Y.append(word2vec_fr.word2vec[word])\n",
    "  \n",
    "X = np.array(X).T\n",
    "Y = np.array(Y).T\n",
    "\n",
    "print(\"X shape : \", X.shape)\n",
    "print(\"Y shape : \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape (300, 300)\n",
      "W shape (300, 300)\n",
      "word2vec_fr_trans shape (50000, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "from scipy import linalg\n",
    "U, S, V = linalg.svd(np.dot(Y, X.T))\n",
    "W = np.dot(U, V)\n",
    "word2vec_fr_trans = np.dot(list(word2vec_fr.word2vec.values()), W)\n",
    "\n",
    "id2word_fr_trans = dict()\n",
    "id2word_fr_list = list(word2vec_fr.word2vec)\n",
    "\n",
    "for id_word in range(len(word2vec_fr_trans)):\n",
    "    id2word_fr_trans[id2word_fr_list[id_word]] = word2vec_fr_trans[id_word]\n",
    "\n",
    "print(\"U shape\", U.shape)\n",
    "print(\"W shape\", W.shape)\n",
    "print(\"word2vec_fr_trans shape\", word2vec_fr_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "souris : ['mouse', 'mice', 'rats', 'rodents', 'raccoons']\n",
      "chat : ['cat', 'rabbit', 'hamster', 'feline', 'poodle']\n",
      "chien : ['dog', 'poodle', 'terrier', 'dogs', 'spaniel']\n",
      "voiture : ['car', 'cars', 'limousine', 'suv', 'roadster']\n",
      "maison : ['house', 'maison', 'townhouse', 'hôtel', 'mansion']\n",
      " --- \n",
      "elephant : ['éléphant', 'elephant', 'éléphants', 'rhinocéros', 'panthère']\n",
      "bed : ['lit', 'matelas', 'lits', 'baignoire', 'sleeping']\n",
      "kitchen : ['cuisines', 'cuisine', 'room', 'kitchen', 'cuisiner']\n",
      "mountain : ['mountain', 'montagne', 'mountains', 'peak', 'montagneuse']\n",
      "cat : ['cat', 'chat', 'dog', 'chats', 'chien']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "def score(word_vec, word_vec_candidate):\n",
    "    return np.dot(word_vec, word_vec_candidate)/(np.linalg.norm(word_vec)*np.linalg.norm(word_vec_candidate))\n",
    "\n",
    "def trans_fr2en(word_to_trans, K = 5):\n",
    "    if word_to_trans not in id2word_fr_trans:\n",
    "        raise ValueError(\"Word not in dic\")\n",
    "    \n",
    "    # Get scores\n",
    "    scores = {}\n",
    "    for word in word2vec_en.word2vec:\n",
    "        word_vec = id2word_fr_trans[word_to_trans]\n",
    "        word_vec_candidate = word2vec_en.word2vec[word]\n",
    "        scores[word] = score(word_vec, word_vec_candidate)\n",
    "    return sorted(scores, key=scores.get)[::-1][0:K]\n",
    "\n",
    "fr = [\"souris\", \"chat\", \"chien\", \"voiture\", \"maison\"]\n",
    "for word in fr:\n",
    "    print(word, \":\", trans_fr2en(word))\n",
    "\n",
    "        \n",
    "def trans_en2fr(word_to_trans, K = 5):\n",
    "    if word_to_trans not in word2vec_en.word2vec:\n",
    "        raise ValueError(\"Word not in dic\")\n",
    "    \n",
    "    # Get scores\n",
    "    scores = {}\n",
    "    for word in id2word_fr_trans:\n",
    "        word_vec = word2vec_en.word2vec[word_to_trans]\n",
    "        word_vec_candidate = np.array(id2word_fr_trans[word])\n",
    "        scores[word] = score(word_vec, word_vec_candidate)\n",
    "    return sorted(scores, key=scores.get)[::-1][0:K]\n",
    "\n",
    "print(\" --- \")\n",
    "\n",
    "fr = [\"elephant\", \"bed\", \"kitchen\", \"mountain\", \"cat\"]\n",
    "for word in fr:\n",
    "    print(word, \":\", trans_en2fr(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "path_train = os.path.join(PATH_TO_DATA,'SST','stsa.fine.train')\n",
    "path_dev = os.path.join(PATH_TO_DATA,'SST','stsa.fine.dev')\n",
    "path_test = os.path.join(PATH_TO_DATA,'SST','stsa.fine.test.X')\n",
    "\n",
    "def read_training(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        x = []\n",
    "        y = []\n",
    "        for line in file:\n",
    "            x.append(line[2:].split())\n",
    "            y.append(int(line[0])) \n",
    "    return x, y\n",
    "\n",
    "def read_test(file_path):\n",
    "    x = []\n",
    "    with open(file_path,'r') as file:\n",
    "        for line in file:\n",
    "            x.append(line.split())\n",
    "    return x\n",
    "\n",
    "x_train, y_train = read_training(path_train)\n",
    "x_dev, y_dev = read_training(path_dev)\n",
    "x_test = read_test(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# Init vocabularies\n",
    "word2vec_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "sentence2vec = BoV(w2v)\n",
    "\n",
    "# Compute idf\n",
    "# idf = sentence2vec.build_idf(x_train + x_dev + x_test)\n",
    "\n",
    "# Encode vectors\n",
    "x_train_enc = sentence2vec.encode(x_train)\n",
    "x_dev_enc = sentence2vec.encode(x_dev)\n",
    "x_test_enc = sentence2vec.encode(x_test)\n",
    "\n",
    "# Encode vectors using idf\n",
    "# x_train_enc_idf = sentence2vec.encode(x_train,idf)\n",
    "# x_dev_enc_idf = sentence2vec.encode(x_dev,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy  0.4328183520599251\n",
      "Dev   set accuracy  0.3751135331516803\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lr = linear_model.LogisticRegression(C= 0.9, max_iter=5000, tol=1e-9, solver='lbfgs', multi_class='auto')\n",
    "lr.fit(x_train_enc,y_train)\n",
    "pred_dev = lr.predict(x_dev_enc)\n",
    "pred_train = lr.predict(x_train_enc)\n",
    "\n",
    "print(\"Train set accuracy \", metrics.accuracy_score(y_train, pred_train))\n",
    "print(\"Dev   set accuracy \", metrics.accuracy_score(y_dev, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# Generating prediction for test vector\n",
    "pred_test = lr.predict(x_test_enc)\n",
    "\n",
    "# Writting the results\n",
    "with open(\"logreg_bov_y_test_sst.txt\",'w') as file:\n",
    "    for line in pred_test:\n",
    "        file.write(str(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "# PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "path_train2 = os.path.join(PATH_TO_DATA,'SST','stsa.fine.train')\n",
    "path_dev2 = os.path.join(PATH_TO_DATA,'SST','stsa.fine.dev')\n",
    "path_test2 = os.path.join(PATH_TO_DATA,'SST','stsa.fine.test.X')\n",
    "\n",
    "x_train2, y_train2 = read_training(path_train2)\n",
    "x_dev2, y_dev2 = read_training(path_dev2)\n",
    "x_test2 = read_test(path_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "import keras.preprocessing as preprocessing\n",
    "import keras.utils as utils\n",
    "\n",
    "words = []\n",
    "for sentence in x_train + x_dev + x_test:\n",
    "    for word in sentence:\n",
    "        if word not in words:\n",
    "            words.append(word)\n",
    "\n",
    "x_train2_one_hot = []\n",
    "x_dev2_one_hot = []\n",
    "x_test2_one_hot = []\n",
    "\n",
    "for x in x_train:\n",
    "    x_train2_one_hot.append(preprocessing.text.one_hot(\" \".join(x), len(words)))\n",
    "\n",
    "for x in x_dev2:\n",
    "    x_dev2_one_hot.append(preprocessing.text.one_hot(\" \".join(x), len(words)))\n",
    "    \n",
    "for x in x_test2:\n",
    "    x_test2_one_hot.append(preprocessing.text.one_hot(\" \".join(x), len(words)))    \n",
    "\n",
    "y_train2 = utils.to_categorical(y_train2)\n",
    "y_dev2 = utils.to_categorical(y_dev2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "import keras.preprocessing.sequence as sequence\n",
    "\n",
    "x_train2_pad = sequence.pad_sequences(x_train2_one_hot)\n",
    "x_dev2_pad = sequence.pad_sequences(x_dev2_one_hot)\n",
    "x_test2_pad = sequence.pad_sequences(x_test2_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 64  # word embedding dimension\n",
    "nhid       = 128  # number of hidden units in the LSTM\n",
    "vocab_size = len(words)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1250304   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,349,765\n",
      "Trainable params: 1,349,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'sgd' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/3\n",
      "8544/8544 [==============================] - 24s 3ms/step - loss: 1.5896 - acc: 0.2670 - val_loss: 1.5797 - val_acc: 0.2534\n",
      "Epoch 2/3\n",
      "8544/8544 [==============================] - 21s 2ms/step - loss: 1.5725 - acc: 0.2711 - val_loss: 1.5742 - val_acc: 0.2534\n",
      "Epoch 3/3\n",
      "8544/8544 [==============================] - 21s 2ms/step - loss: 1.5688 - acc: 0.2722 - val_loss: 1.5732 - val_acc: 0.2534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 3\n",
    "\n",
    "history = model.fit(x_train2_pad, y_train2, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev2_pad, y_dev2))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()\n",
    "\n",
    "# NOTE : model training are too stressfull operations for my computer which is smelling like burnt electric cables...\n",
    "# even if I would enjoy to better fit the model and improve my results, I can't afford to break my computer\n",
    "# as it already happened to me before. I hope you will understand my concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "model_pred = model.predict(x_test2_pad, batch_size=bs).argmax(axis=-1)\n",
    "model_pred.tofile('logreg_lstm_y_test_sst.txt', sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
